apiVersion: "sparkoperator.hpe.com/v1beta2"
kind: SparkApplication
metadata:
  name: "spark-etl"
  namespace: "spark"
spec:
  sparkConf:
    # S3 credentials should be specified below
    #spark.hadoop.fs.s3a.endpoint: "https://objectstore-zone1-svc.dataplatform.svc.cluster.local:9000/"
    spark.hadoop.fs.s3a.endpoint: "https://m2-ps-vm196.mip.storage.hpecorp.net:9000/"
    spark.hadoop.fs.s3a.access.key: "8I32FX0VMHE0SY4299OQ71C59F44TV1H463DUK3Y5SW6ECQHY9QZMPPS8KSTTY7QDEAUUYA0P8JPWJ3CGS8P9QQFINYDHZJ4W7X609VVF4J63ECIL"
    spark.hadoop.fs.s3a.secret.key: "6ESX3SWZGO2UD6AYBI1WG72JK74R5Z97S80QZZI3H29T9ZCVWWSGVR5TEOH5K1MZVBE0KWVWTOM4WNRWFZV1TLY8W0LLB9BCZT68J6"
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    spark.hadoop.fs.s3a.connection.ssl.enabled: "true"

    # Needed if s3 server is behind proxy
    # spark.hadoop.fs.s3a.proxy.host: "web-proxy.corp.hpecorp.net"
    # spark.hadoop.fs.s3a.proxy.port: "8080"

    # Needed if s3 server certs are not imported to java truststore used by spark
    spark.driver.extraJavaOptions: "-Dcom.amazonaws.sdk.disableCertChecking=true"
    spark.executor.extraJavaOptions: "-Dcom.amazonaws.sdk.disableCertChecking=true"
  type: Python
  sparkVersion: 3.3.1
  mode: cluster
  image: "gcr.io/mapr-252711/spark-py-3.3.1:v3.3.1"
  imagePullPolicy: Always
  mainApplicationFile: "local:///mounts/shared-volume/spark/spark_etl_new.py"
  restartPolicy:
    type: Never
  imagePullSecrets:
    - imagepull
  driver:
    labels:
      version: 3.3.1
    cores: 1
    coreLimit: "1"
    memory: 1g
    serviceAccount: hpe-spark
    volumeMounts:
      - name: {{dag_run.conf.get("username")}}-volume
        mountPath: /mounts/{{dag_run.conf.get("username")}}-volume
      - name: shared-volume
        mountPath: /mounts/shared-volume
  executor:
    labels:
      version: 3.3.1
    cores: 1
    coreLimit: "1"
    memory: 1g
    serviceAccount: hpe-spark
    volumeMounts:
      - name: {{dag_run.conf.get("username")}}-volume
        mountPath: /mounts/{{dag_run.conf.get("username")}}-volume
      - name: shared-volume
        mountPath: /mounts/shared-volume
    instances: 1
  volumes:
    - name: {{dag_run.conf.get("username")}}-volume
      persistentVolumeClaim:
        claimName: {{dag_run.conf.get("username")}}-spark-pvc
    - name: shared-volume
      persistentVolumeClaim:
        claimName: kubeflow-shared-pvc
